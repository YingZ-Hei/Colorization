{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aac31b-3c80-4086-86ac-b4f0cfdb7c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform, color\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae77be-16fe-42b1-9523-4604be2a7bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImagePath_flower = \"flower/\"\n",
    "ImagePath_face = \"aniF/\"\n",
    "ImagePath_landscape = \"landscape/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38600818-0282-4f41-803d-2198dd9b5c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "\n",
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.image_list = [i for i in os.listdir(path) if int(i.split('.')[0]) < 1500]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_list[idx]\n",
    "\n",
    "        # original image\n",
    "        img_rgb_orig = io.imread(os.path.join(self.path, image_name))\n",
    "\n",
    "        # Resize\n",
    "        img_rgb_res = transform.resize(img_rgb_orig, (HEIGHT, WIDTH), anti_aliasing=True)\n",
    "\n",
    "        # Convert to Lab color space\n",
    "        img_lab_res = color.rgb2lab(img_rgb_res)\n",
    "\n",
    "        # Separate the L and ab channels\n",
    "        l_res = img_lab_res[:, :, 0]\n",
    "        ab_res = img_lab_res[:, :, 1:]\n",
    "\n",
    "        # Normalize L and ab values\n",
    "        # l_orig = l_orig / 100.0  # Scale L channel between 0 and 1\n",
    "        l_res = l_res / 100.0  # Scale L channel between 0 and 1\n",
    "        ab_res = ab_res / 128.0  # Scale ab channel between -1 and 1\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        l_res_tensor = torch.tensor(l_res, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "        ab_res_tensor = torch.tensor(ab_res, dtype=torch.float32).permute(2, 0, 1)  # Change shape to (C, H, W)\n",
    "\n",
    "        # sample = {'L_orig': l_orig_tensor, 'L_res': l_res_tensor, 'ab': ab_res_tensor}\n",
    "        sample = {'L_res': l_res_tensor, 'ab': ab_res_tensor}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39ded7a-40d8-4f0b-93a8-af9bad344160",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorizationResNet\n",
    "              , self).__init__()\n",
    "\n",
    "        # Load pretrained ResNet-18\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        # Modify first layer to accept 1-channel input (grayscale L channel)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False),  # Change input to 1 channel\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,\n",
    "            resnet.layer2,\n",
    "            resnet.layer3,\n",
    "            resnet.layer4\n",
    "        )\n",
    "\n",
    "        # Decoder (More upsampling layers to restore 256x256)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),  # 8x8 → 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 16x16 → 32x32\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 32x32 → 64x64\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),    # 64x64 → 128x128\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),    # 128x128 → 256x256\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 2, kernel_size=1),  # Output 2-channel (ab)\n",
    "            nn.Tanh()  # Normalize to [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, input_l):\n",
    "        x = self.encoder(input_l) \n",
    "        x = self.decoder(x)  # Upsample to (2, 256, 256)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0840ee0-5337-48b2-9fb9-a17fbad11e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0)\n",
    "\n",
    "def train_colorization_model(model, dataset, epochs, batch_size, learning_rate, device='cuda'):\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    l1_loss = nn.L1Loss()\n",
    "    mse_loss = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            # Get the inputs and labels from the batch\n",
    "            l_res = batch['L_res'] # L channel (input)\n",
    "            ab_res = batch['ab']   # ab channels (target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(l_res)\n",
    "            # #run this if use mse\n",
    "            # loss = mse_loss(outputs, ab_res)\n",
    "            # #run this if use l1\n",
    "            # loss = l1_loss(outputs, ab_res) \n",
    "            # #run this if use ssim\n",
    "            loss = 1 - ssim_metric(outputs, ab_res)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Average Loss: {avg_loss:.4f}\")\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return model, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7d748d-7587-4602-a5ab-29c6ddd5d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataset = ColorizationDataset(ImagePath_flower)\n",
    "ResNet = ColorizationResNet()\n",
    "model, losses = train_colorization_model(ResNet, dataset, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cc",
   "language": "python",
   "name": "cc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
